{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-03 13:49:27.611569: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /jet/home/rni/ECE_STORAGE/tfds/librispeech/2.1.1...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc73e303e42242249aa1d1c6e080a209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1826404ae4349bbb472ffe743539dc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c26d5ddbe645d890491ff6541ea587",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extraction completed...: 0 file [00:00, ? file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'apache_beam'\nFailed importing apache_beam. This likely means that the dataset requires additional dependencies that have to be manually installed (usually with `pip install apache_beam`). See setup.py extras_require.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/lazy_imports_lib.py:30\u001b[0m, in \u001b[0;36m_try_import\u001b[0;34m(module_name)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m   mod \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39;49mimport_module(module_name)\n\u001b[1;32m     31\u001b[0m   \u001b[39mreturn\u001b[39;00m mod\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[39mreturn\u001b[39;00m _bootstrap\u001b[39m.\u001b[39;49m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1004\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'apache_beam'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ds \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mlibrispeech\u001b[39;49m\u001b[39m\"\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest_clean\u001b[39;49m\u001b[39m'\u001b[39;49m, data_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/jet/home/rni/ECE_STORAGE/tfds\u001b[39;49m\u001b[39m'\u001b[39;49m, download\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:250\u001b[0m, in \u001b[0;36mload.<locals>.decorator\u001b[0;34m(function, unused_none_instance, args, kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m name \u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    249\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    251\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    252\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:575\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[39mif\u001b[39;00m download:\n\u001b[1;32m    574\u001b[0m   download_and_prepare_kwargs \u001b[39m=\u001b[39m download_and_prepare_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 575\u001b[0m   dbuilder\u001b[39m.\u001b[39;49mdownload_and_prepare(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m as_dataset_kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    578\u001b[0m   as_dataset_kwargs \u001b[39m=\u001b[39m {}\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:523\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[1;32m    521\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mread_from_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir)\n\u001b[1;32m    522\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    524\u001b[0m       dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    525\u001b[0m       download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m    526\u001b[0m   )\n\u001b[1;32m    528\u001b[0m   \u001b[39m# NOTE: If modifying the lines below to put additional information in\u001b[39;00m\n\u001b[1;32m    529\u001b[0m   \u001b[39m# DatasetInfo, you'll likely also want to update\u001b[39;00m\n\u001b[1;32m    530\u001b[0m   \u001b[39m# DatasetInfo.read_from_directory to possibly restore these attributes\u001b[39;00m\n\u001b[1;32m    531\u001b[0m   \u001b[39m# when reading from package data.\u001b[39;00m\n\u001b[1;32m    532\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdownload_size \u001b[39m=\u001b[39m dl_manager\u001b[39m.\u001b[39mdownloaded_size\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:1249\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1244\u001b[0m split_generators \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_generators(  \u001b[39m# pylint: disable=unexpected-keyword-arg\u001b[39;00m\n\u001b[1;32m   1245\u001b[0m     dl_manager, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptional_pipeline_kwargs)\n\u001b[1;32m   1246\u001b[0m \u001b[39m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[39m# https://github.com/tensorflow/datasets/issues/2537\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[39m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001b[39;00m\n\u001b[0;32m-> 1249\u001b[0m split_generators \u001b[39m=\u001b[39m split_builder\u001b[39m.\u001b[39;49mnormalize_legacy_split_generators(\n\u001b[1;32m   1250\u001b[0m     split_generators\u001b[39m=\u001b[39;49msplit_generators,\n\u001b[1;32m   1251\u001b[0m     generator_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_examples,\n\u001b[1;32m   1252\u001b[0m     is_beam\u001b[39m=\u001b[39;49m\u001b[39misinstance\u001b[39;49m(\u001b[39mself\u001b[39;49m, BeamBasedBuilder),\n\u001b[1;32m   1253\u001b[0m )\n\u001b[1;32m   1255\u001b[0m \u001b[39m# Ensure `all` isn't used as key.\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m _check_split_names(split_generators\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/split_builder.py:267\u001b[0m, in \u001b[0;36mSplitBuilder.normalize_legacy_split_generators\u001b[0;34m(self, split_generators, generator_fn, is_beam)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(split_generators, \u001b[39mlist\u001b[39m):  \u001b[39m# Legacy structure\u001b[39;00m\n\u001b[1;32m    266\u001b[0m   \u001b[39mif\u001b[39;00m is_beam:  \u001b[39m# Legacy `tfds.core.BeamBasedBuilder`\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     beam \u001b[39m=\u001b[39m lazy_imports_lib\u001b[39m.\u001b[39;49mlazy_imports\u001b[39m.\u001b[39;49mapache_beam\n\u001b[1;32m    268\u001b[0m     generator_fn \u001b[39m=\u001b[39m beam\u001b[39m.\u001b[39mptransform_fn(generator_fn)\n\u001b[1;32m    269\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m    270\u001b[0m         s\u001b[39m.\u001b[39mname: generator_fn(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39ms\u001b[39m.\u001b[39mgen_kwargs)  \u001b[39m# Create the `beam.PTransform`\u001b[39;00m\n\u001b[1;32m    271\u001b[0m         \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m split_generators\n\u001b[1;32m    272\u001b[0m     }\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/utils/py_utils.py:153\u001b[0m, in \u001b[0;36mclassproperty.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get__\u001b[39m(\u001b[39mself\u001b[39m, obj, objtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfget\u001b[39m.\u001b[39;49m\u001b[39m__get__\u001b[39;49m(\u001b[39mNone\u001b[39;49;00m, objtype)()\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/lazy_imports_lib.py:51\u001b[0m, in \u001b[0;36mLazyImporter.apache_beam\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m@utils\u001b[39m\u001b[39m.\u001b[39mclassproperty\n\u001b[1;32m     49\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapache_beam\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[0;32m---> 51\u001b[0m   \u001b[39mreturn\u001b[39;00m _try_import(\u001b[39m\"\u001b[39;49m\u001b[39mapache_beam\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/lazy_imports_lib.py:37\u001b[0m, in \u001b[0;36m_try_import\u001b[0;34m(module_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     33\u001b[0m   err_msg \u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFailed importing \u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m. This likely means that the dataset \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     34\u001b[0m              \u001b[39m\"\u001b[39m\u001b[39mrequires additional dependencies that have to be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m              \u001b[39m\"\u001b[39m\u001b[39mmanually installed (usually with `pip install \u001b[39m\u001b[39m{name}\u001b[39;00m\u001b[39m`). See \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m              \u001b[39m\"\u001b[39m\u001b[39msetup.py extras_require.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mmodule_name)\n\u001b[0;32m---> 37\u001b[0m   utils\u001b[39m.\u001b[39;49mreraise(e, suffix\u001b[39m=\u001b[39;49merr_msg)\n",
      "File \u001b[0;32m/ocean/projects/cie160013p/rni/785-proj/PWGAN-ASR-defense/offline_defense/utils/.conda/envs/tf/lib/python3.10/site-packages/tensorflow_datasets/core/utils/py_utils.py:381\u001b[0m, in \u001b[0;36mreraise\u001b[0;34m(e, prefix, suffix)\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    380\u001b[0m     exception \u001b[39m=\u001b[39m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(e)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 381\u001b[0m   \u001b[39mraise\u001b[39;00m exception \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# Otherwise, modify the exception in-place\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(e\u001b[39m.\u001b[39margs) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'apache_beam'\nFailed importing apache_beam. This likely means that the dataset requires additional dependencies that have to be manually installed (usually with `pip install apache_beam`). See setup.py extras_require."
     ]
    }
   ],
   "source": [
    "ds = tfds.load(\"librispeech\", split='test_clean', data_dir='/jet/home/rni/ECE_STORAGE/tfds', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f42adc9e40d25111417e42bd551e1c22ef0dbf31b190e9931ea2086457d4cfa7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
